# Классификация временных рядов

До этого момента мы говорили про задачу прогнозирования временных рядов, которая сводится к задаче регрессии. Однако временные ряды также возможно классифицировать. Этот подход активно используется в медицине. Например, можно автоматически выявлять заболевания по ЭКГ и ЭЭГ. Также задача классификации встречается в умных часах, когда определяет разные типы активности человека по данным с акселерометра (ходьба, бег, ходьба по лестнице).

Обычно задача формулируется так - дан датасет временных рядов (в общем случае многомерных) разной длины, и каждому ряду сопоставлена метка класса. Нужно построить функцию, которая для нового ряда даст прогноз его метки класса. Оценка качества в данном случае будет похожа на обычную оценку качества моделей классификации, и можно использовать все те же accuracy, precision и recall, а вот модели будут немного другими.

Почему классические модели классификации не подходят для временных рядов? Все дело в том, что во временном ряду наблюдения связаны между собой, а признаки у одного объекта в классическом машинном обучении могут быть зависимы, а могут быть и независимыми. Наивный байесовский классификатор, например, предполагает, что признаки независимы и его использовать точно нельзя. А вот дерево решений или логистическая регрессия таких предположений не делает и их использовать можно, но результат будет значительно хуже, чем если использовать специализированные модели.

Все алгоритмы, о которых сегодня пойдет речь, реализованы в библиотеке [`sktime`](https://www.sktime.org/en/stable/), которая позволяет работать с временными рядами. [Инструкция по установке](https://www.sktime.org/en/stable/installation.html). Практическое применение алгоритмов будет в видео после этого урока, а пока давайте разберемся с моделями.

## kNN

При анализе задачи классификации мы говорили про модель K ближайших соседей. Эта модель ищет примеры в датасете, которые расположены максимально близко к нужной точке и путем голосования определяет какому классу принадлежит точка. Похожую идею можно переложить на временные ряды. Но есть проблема - временные ряды в обучающей выборке могут быть разной длины, в то время как у объектов в обычной задачи классификации фиксированное количество признаков. Нам было важно фиксированное количество признаков, потому что мы считали евклидово расстояние между объектами. Евклидово расстояние предполагает, что все координаты независимы между собой, а нам это не подходит. Давайте выберем другой способ измерения расстояния для временных рядов.

Что должна учитывать новая метрика? Во-первых, как мы выяснили, ей должно быть все равно на размерность двух входящих величин. Во-вторых, нам бы хотелось, чтобы расстояние между геометрически похожими между собой временными рядами было меньше, чем между непохожими (сюда же закладывается зависимость между переменными). Например, если мы хотим классифицировать ходьбу у двух разных людей, то у обоих людей график акселерометра по одной из осей будет выглядеть как синусоида, но ее частота будет разной, так как люди могут быть разного роста и могут ходить в разном темпе, но нам бы хотелось относить их к одному классу. Одна из метрик, подходящих под все эти качества - [DTW (dynamic time warp)](https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%B4%D0%B8%D0%BD%D0%B0%D0%BC%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B9_%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%B8_%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D0%BE%D0%B9_%D1%88%D0%BA%D0%B0%D0%BB%D1%8B).

В основе алгоритма все равно будет лежать евклидово расстояние. Главной проблемой евклидова расстояния для нас является то, что при небольшом сдвиге одного ряда, метрика сильно ухудшится. Нам бы хотелось "сжимать" и "разжимать" ось X в нужных местах, чтобы значение метрики не ухудшалось.

Используя в kNN вместо евклидова расстояние DTW, можно неплохо классифицировать временные ряды.

## Time Series Forest (TSF)

Основная идея метода такая:

1. Разбиваем временной ряд на рандомные интервал с произвольной точкой старта и произвольной длиной
2. Извлекаем из каждого интервала следующие значения - среднее, среднеквадратичное отклонение, наклон. Записываем это в специальный вектор. Один временной ряд - один вектор.
3. Обучаем решающее дерево на таком датасете.
4. Повторяем 1-3, строя множество деревьев, и получая таким образом случайный лес, построенный на временных рядах.
5. Прогноз получается благодаря голосованию, как и в обычном случайном лесе.

Скорость работы по большей части зависит от количества деревьев.

## Методы классификации, основанные на словарях

Существует множество алгоритмов, основанных на трансформации временного ряда в последовательность символов, которую используют для классификации. Общий принцип для всех алгоритмов такой - скользящим окном длины `w` мы проходим по всему временному ряду, и на каждом шаге формируется "слово" длины `l`, и причем всего используется фиксированное количество букв. В итоге получается гистограмма, которую можно использовать для классификации.

Методы, которые используют такой подход:

- BOSS
- BOSS Ensemble
- BOSS Contractable

## Какой метод выбрать?

Как и в классическом машинном обучении, заранее сказать нельзя. Нужно просто попробовать несколько методов и понять какой из них на ваших данных работает лучше всего. Также можно строить стекинги из всех перечисленных методов. Если вы знаете какие-то особенности про вашу задачу, то, возможно, и эти модели вам не понадобятся, а задача решится обычной логистической регрессией.
