# Модели классификации

Давайте рассмотрим еще несколько моделей машинного обучения для задачи классификации

## kNN

Как понять к какому классу принадлежит точка, если известно к какому классу принадлежат другие точки в пространстве? Можно просто найти ближайших соседей этой точки и посмотреть соседей какого класса больше. В этом и заключается суть метода kNN (k ближайших соседей).

Давайте посмотрим как это реализовывается в Python

```python
from sklearn.neighbors import KNeighborsClassifier

```

kNN хорошо работает, если данные нормализованы и колонок не очень много. Если колонок будет много, то мы столкнемся с [проклятием размерности](http://www.machinelearning.ru/wiki/index.php?title=%D0%9F%D1%80%D0%BE%D0%BA%D0%BB%D1%8F%D1%82%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8). Чем больше признаков в данных, тем больше необходимо данных, чтобы построить качественную модель, и причем необходимо количество данных растет экспоненциально.

Также стоит отметить, что kNN не требуется обучение как таковое, так как у метода нет никаких параметров (кроме количества соседей и метрики, по которой считаем расстояние).

## [Наивный байесовский классификатор](https://habr.com/ru/post/120194/)

НБК - это простой, но довольно полезный алгоритм классификации, который полностью построен на теореме Байеса.

Пусть у нас есть элемент $x$, который может пренадлежать одному из классов $c \in C$. Тогда нам нужно при какой метке класса вероятность принадлежности будет максимальная:

$$c_{opt} = argmax_{c} P(c|x)$$

Используя теорему Байеса вероятность можно переписать так:

$$P(c|x) = \frac {P(x|c)P(c)} {P(x)}$$

Вектор $x$ состоит из признаков $x_1, \dots, x_n$, поэтому формулу можно записать так:

$$P(c|x_1 \dots x_n) = \frac {P(x_1 \dots x_n | c) P(c)} {P(x_1 \dots x_n)}$$

Значение в знаменателе нас не интересует, так как это константа, а нам нужно найти максимум.

Выражение выглядит довольно сложно, поэтому сделаем предположение, что признаки между собой независимы (например, возраст и пол пассажира на титанике, или значение в первом и втором пикселе на картинке). Тогда можно переписать числитель так:

$$P(x_1 \dots x_n | c) P(c) = P(c) P(x_1|c) P(x_2 | c) \dots P(x_n| c) = P(c) \prod_{i} P(x_i|c)$$

Запишем исходную задачу, используя эти формулы:

$$c_{opt} = argmax_{c} = argmax_{c} P(c) \prod_{i} P(x_i|c)$$

Все эти значения мы можем посчитать:

- $P(c)$ - вероятность встретить класс в выборке
- $P(x_i|c)$ - вероятность, что встретим значение $x_i$ при классе $c$

Подсчет этих значений и есть обучение классификатора.

Алгоритм требует немного данных для работы, довольно быстро работает (градиентных спусков нет, хранить в памяти тоже почти ничего не нужно) и часто обходит в точности другие, более сложные, алгоритмы (деревья решений и логистическую регрессию, например).

Также немаловажный плюс - можно задать априорные вероятности классов $P(c)$, если у нас скошенная выборка, либо мы знаем об ошибке в сборе данных.

Однако есть недостаток - алгоритм предполагает, что признаки независимы, а это не так в случае со словами в тексте, например.

**добавить пример**

```python
from sklearn.naive_bayes import GaussianNB

```

## Support Vector Machine (SVM)

В базовой версии этот алгоритм очень похож на логистическую регрессию - мы строим разделяющую прямую, которая разделяет плоскость на две части.

Классифицирующая функция выглядит так: $F(x) = sign((w, x) + b)$. То есть, возвращает $+1$ или $-1$, в зависимости от того, какому классу принадлежит объект $x$.

Часто мы можем построить несколько прямых, поэтому мы хотим найти прямую с максимальным расстоянием до выборки.

![svm.png](../images/svm.png)

Можно показать, что расстояние до выборки равно $\frac{1}{||w||}$. Вместо задачи максимизации обычно берут задачу минимизации и минимизируют $||w||^2$, что является эквивалентной задачей.

Формально это можно записать так:

$argmin ||w||^2$

$y_i ((w, x_i) + b) \geq 1$

С последним уравнением стоит разобраться. Здесь нам интересны знаки $y_i$ и $(w, x_i) + b$. Если эти выражения будут иметь одинаковые знаки (либо оба плюса, либо оба минуса), то мы правильно решили задачу классификации. Если знаки отличаются, то результат получится отрицательным, и задача решена неверно.

В sklearn этот метод называется `SVC` - support vector classifier. Также есть алгоритм кластеризации support vector clustering, но он не реализован в sklearn.

**переделать пример**

```python
model = SVC(kernel='linear')

n = 100
x = np.random.normal(size=(n, 2))
y = np.random.randint(0, 2, size=(n,))

model.fit(x, y)

x1_plot = np.linspace(-3, 3, 100)
x2_plot = np.linspace(-3, 3, 100)
x1_mesh, x2_mesh = np.meshgrid(x1_plot, x2_plot)
x_plot = np.vstack((x_mesh.ravel(), y_mesh.ravel())).T

res = model.predict(x_plot).reshape(x1_mesh.shape)
plt.contourf(x1_mesh, x2_mesh, res, alpha=0.8)
plt.scatter(x[:, 0], x[:, 1], c=y, edgecolor='black')
```

Как можно заметить, если выборка не разделяется линейно, то модель работает не слишком хорошо. В таком случае используют другие ядра - например, `rbf` и `poly`.

Математически это означает, что в функции $F(x) = sign((w, \phi (x)) + b)$ вводится новая функция $\phi(x)$, которая преобразует исходные данные к более простому для модели виду.

В случае с линейным ядром, эта функция равна $\phi(x) = x$.

### Виды ядер

Есть три основных вида ядер: линейное, полиномиальное и RBF.

**Линейное ядро.**
Это самый простой тип ядра, обычно одномерный по своей природе. Это оказывается лучшей функцией, когда есть много признаков. Линейное ядро в основном предпочтительнее для задач классификации текста, поскольку большинство таких задач классификации можно разделить линейно.
Линейные функции ядра работают быстрее, чем другие функции.

![svm_linear.png](../images/linear_kernel.png)

**Полиномиальное ядро.**
Это более обобщенное представление линейного ядра. Обычно работает немного точнее линейного ядра, но и считается дольше.

![svm_poly.png](../images/poly_kernel.png)

**RBF(радиальная базисная функция) ядро.**
Это одна из наиболее предпочтительных и используемых функций ядра в svm. Обычно его выбирают для нелинейных данных. Это помогает сделать правильное разделение, когда нет предварительных знаний о данных.

![svm_rbf.png](../images/rbf_kernel.png)

## Дерево решений

Решающие деревья - класс моделей, которые очень похожи на то, как человек принимает решения.

Решающее дерево состоит из узлов (веток), в которых данные классифицируются и листьев, в которых выводится результат классификации

![decision_tree.jpg](attachment:decision_tree.jpg)

Понять как это работает можно на примере игры в данетки

![%D0%94%D0%B0%D0%BD%D0%B5%D1%82%D0%BA%D0%B8.png](attachment:%D0%94%D0%B0%D0%BD%D0%B5%D1%82%D0%BA%D0%B8.png)

Секрет успеха в том, чтобы задавать сначала общие вопросы, а потом уже более конкретные.

Решающее дерево строится на основе этого секрета - все вопросы, на которые дается ответ это узлы дерева.

### Использование модели

Как и в случае с остальными моделями, можно использовать готовое решение - класс `DecisionTreeClassifier` для классификации.

Также деревья решений обобщаются на задачу регрессии, и в таком случае используют `DecistionTreeRegressor`.

### Практика

Давайте возьмем набор данных xor

Обучим на нем решающее дерево и посмотрим что получится

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from mlxtend.plotting import category_scatter, plot_decision_regions

data = pd.read_csv('../data/xor.csv', sep=';')
x_train, x_test, y_train, y_test = train_test_split(data[['X', 'Y']], data['class'], train_size=0.8)
```

Посмотрим на данные

```python
category_scatter('X', 'Y', 'class', data)
plt.show()
```

Обучим модель

```python
model = DecisionTreeClassifier()
model.fit(x_train, y_train)

train_prediction = model.predict(x_train)
test_prediction = model.predict(x_test)

train_accuracy = accuracy_score(train_prediction, y_train)
test_accuracy = accuracy_score(test_prediction, y_test)

print('Точность на обучающей выборке: ', train_accuracy)
print('Точность на тестовой выборке: ', test_accuracy)
```

```python
plt.figure(figsize=(40,20))  
plot_tree(model, feature_names = x_test.columns, 
             filled=True, fontsize=9, rounded = True)
plt.show()

```

Алгоритм переобучился, почему так произошло?

## Как это работает?

Для простоты рассмотрим задачу бинарной классификации.

### Условия в узлах

Решающее дерево состоит из узлов, в которых проверяется какое-то условие.

На практике чаще всего берут условие $x_j < t$, то есть сравнивают значение какого-то признака с порогом.

Также это означает, что мы разбиваем пространство признаков на зоны при помощи прямых линий.

![tree_example.png](attachment:tree_example.png)

### Разбиение выборки

Формально, будем разбивать выборку $X$ на две подвыборки - $L$ и $R$ с условиями $x_j < t$ и $x_j >= t$

Насколько хорошо мы разбили выборку можно измерить при помощи специального функционала качества $Q(X, j, t)$, который обсудим позже.

Будем разбивать подвыборки до тех пор, пока не выполнится критерий останова. Типичные критерии останова:

- остался один элемент в выборке
- все элементы принадлежат одному классу
- достигли максимальной установленной глубины

Итак, мы построили дерево и в последнем листе у нас лежат какие-то элементы выборок (не обязательно все принадлежат одному классу). Возьмем за ответ самый популярный класс в листе.

### Функционал качества

Функционал качества определяет насколько мы хорошо разбили выборку в каждом узле.

Обычно его записывают так:

$Q(X, j, t) = \frac{|L|}{|X|} H(L) + \frac{|R|}{|X|} H(R)$

где $H(X)$ - это критерий информативности, который оценивает однородность распределения целевой переменной.

Для задачи классификации $H(X)$ можно взять таким:

$H(X) = \sum p_k (1 - p_k)$

Этот критерий информативности называется [критерием Джини](https://habr.com/ru/company/ods/blog/350440/).

Также иногда используют энтропийный критерий

$H(X) = -\sum p_k log p_k$

### Обучение модели

Обучение происходит за счет подбора оптимальных параметров $j$ и $t$ на каждом шаге разбиения дерева.

$j$ подбирается перебором, $t$ - бинарным поиском

Если не задать ограничения на глубину дерева или колличество обьектов в листе, дерево будет строиться до тех пор, пока не найдет идеальное разделение для каждого обьекта!

Из структуры дерева решений следует несколько интересных свойств:

- выученная функция является кусочно-постоянной, из-за чего производная равна нулю везде, где задана. Следовательно, о градиентных методах при поиске оптимального решения можно забыть;
- дерево решений (в отличие от, например, линейной модели) не сможет экстраполировать зависимости за границы области значений обучающей выборки;
- дерево решений способно идеально приблизить обучающую выборку и ничего не выучить (то есть такой классификатор будет обладать низкой обобщающей способностью): для этого достаточно построить такое дерево, в каждый лист которого будет попадать только один объект. Следовательно, при обучении нам надо не просто приближать обучающую выборку как можно лучше, но и стремиться оставлять дерево как можно более простым, чтобы результат обладал хорошей обобщающей способностью.
