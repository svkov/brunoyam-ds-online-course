# Модели классификации

Давайте рассмотрим еще несколько моделей машинного обучения для задачи классификации

## kNN

Как понять к какому классу принадлежит точка, если известно к какому классу принадлежат другие точки в пространстве? Можно просто найти ближайших соседей этой точки и посмотреть соседей какого класса больше. В этом и заключается суть метода kNN (k ближайших соседей).

Давайте посмотрим как это реализовывается в Python

```python
from sklearn.neighbors import KNeighborsClassifier

```

kNN хорошо работает, если данные нормализованы и колонок не очень много. Если колонок будет много, то мы столкнемся с [проклятием размерности](http://www.machinelearning.ru/wiki/index.php?title=%D0%9F%D1%80%D0%BE%D0%BA%D0%BB%D1%8F%D1%82%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8). Чем больше признаков в данных, тем больше необходимо данных, чтобы построить качественную модель, и причем необходимо количество данных растет экспоненциально.

Также стоит отметить, что kNN не требуется обучение как таковое, так как у метода нет никаких параметров (кроме количества соседей и метрики, по которой считаем расстояние).

## [Наивный байесовский классификатор](https://habr.com/ru/post/120194/)

НБК - это простой, но довольно полезный алгоритм классификации, который полностью построен на теореме Байеса.

Пусть у нас есть элемент $x$, который может пренадлежать одному из классов $c \in C$. Тогда нам нужно при какой метке класса вероятность принадлежности будет максимальная:

$$c_{opt} = argmax_{c} P(c|x)$$

Используя теорему Байеса вероятность можно переписать так:

$$P(c|x) = \frac {P(x|c)P(c)} {P(x)}$$

Вектор $x$ состоит из признаков $x_1, \dots, x_n$, поэтому формулу можно записать так:

$$P(c|x_1 \dots x_n) = \frac {P(x_1 \dots x_n | c) P(c)} {P(x_1 \dots x_n)}$$

Значение в знаменателе нас не интересует, так как это константа, а нам нужно найти максимум.

Выражение выглядит довольно сложно, поэтому сделаем предположение, что признаки между собой независимы (например, возраст и пол пассажира на титанике, или значение в первом и втором пикселе на картинке). Тогда можно переписать числитель так:

$$P(x_1 \dots x_n | c) P(c) = P(c) P(x_1|c) P(x_2 | c) \dots P(x_n| c) = P(c) \prod_{i} P(x_i|c)$$

Запишем исходную задачу, используя эти формулы:

$$c_{opt} = argmax_{c} = argmax_{c} P(c) \prod_{i} P(x_i|c)$$

Все эти значения мы можем посчитать:

- $P(c)$ - вероятность встретить класс в выборке
- $P(x_i|c)$ - вероятность, что встретим значение $x_i$ при классе $c$

Подсчет этих значений и есть обучение классификатора.

Алгоритм требует немного данных для работы, довольно быстро работает (градиентных спусков нет, хранить в памяти тоже почти ничего не нужно) и часто обходит в точности другие, более сложные, алгоритмы (деревья решений и логистическую регрессию, например).

Также немаловажный плюс - можно задать априорные вероятности классов $P(c)$, если у нас скошенная выборка, либо мы знаем об ошибке в сборе данных.

Однако есть недостаток - алгоритм предполагает, что признаки независимы, а это не так в случае со словами в тексте, например.

**добавить пример**

```python
from sklearn.naive_bayes import GaussianNB

```

