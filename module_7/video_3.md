# Стохастический градиентный спуск

В теоретическом материале мы рассказывали о том, что такое градиентный спуск, как его запрограммировать и какие есть тонкости при работе с ним. Сейчас давайте посмотрим на интерактивную визуализацию, чтобы еще раз закрепить материал.

[Ссылка на графики](https://www.benfrederickson.com/numerical-optimization/)

Перед нами линии уровня графика функции. Более темные области соответствуют меньшим значениям, мы хотим найти минимум.

Как видно, здесь есть 4 локальных минимума. В зависимости от стартовой точки, мы можем попасть в один из них.

Также мы можем выбрать learning rate. Если мы выберем слишком большой learning rate, то алгоритм разойдется. Если выберем слишком маленький, то итераций будет больше, чем нам бы хотелось.

Также тут есть возможность выбирать оптимальную длину шага для каждой итерации. Для этого нужно поставить галочку "Use line search".

На некоторых функциях градиентный спуск может работать не очень хорошо. С этим мы, к сожалению, ничего не можем поделать - остается только подбирать параметры и надеяться на лучшее.
