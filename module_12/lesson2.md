# Задача понижения размерности

В задачах обработки текстов и биоинформатики часто размерность пространства становится очень большой и может превышать даже количество примеров. Обучать модели машинного обучения на таких данных сложно и неэффективно. Благодаря алгоритмам понижения размерности мы можем отобразить многомерные данные в пространство меньшей размерности, потеряв минимум информации. Самым известным алгоритмом понижения размерности является **метод главных компонент** (PCA - Principal Component Analysis)

## Метод главных компонент

Суть метода заключается в том, чтобы выбрать такую плоскость, на которую можно спроецировать данные, потеряв минимум информации.

На вход алгоритму поступает датасет из $n$ точек и с $k$ признаками и количество размерностей у сжатого датасета $m$, а на выходе алгоритм выдает датасет с $m$ признаками.

![pca](../images/pca.png)

Для того, чтобы найти такую плоскость, потребуется продвинутый математический аппарат. [Здесь](https://habr.com/ru/post/304214/) есть неплохое описание того, как это в действительности работает. Мы же посмотрим как использовать этот алгоритм в `sklearn`.

```python

```

Алгоритм умеет оценивать какой процент информации потеряется при сжатии, это называется **необъясненная дисперсия**. Разделим необъясненную дисперсию на всю дисперсию в датасете и получим **процент необъясненной дисперсии**. Если мы не знаем какая размерность нужна у нового датасета, то мы можем указать **процент объясненной дисперсии** (100% - процент необъясненной дисперсии) и алгоритм сам подберет количество координат.

Например, мы хотим оставить 80% информации в данных, тогда мы вызовем метод с параметров 0.8.

```python

```
