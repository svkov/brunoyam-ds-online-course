# Генеративно-состязательные нейросети

Существует вид нейросетей, которые позволяют генерировать изображения на основе обучающей выборки. Это unsupervised подход, то есть нам не нужен таргет для этой задачи.

Для того, чтобы сгенерировать изображение, нам нужна сверточная нейросеть, которая на вход будет принимать какой-то случайный вектор, а на выходе будет выдавать изображение. Но как обучить такую нейросеть?

## Генератор и дискриминатор

Обычно генеративные нейросети обучают при помощи второй нейросети - дискриминатора. Во время обучения обе нейросети работают в тандеме и общий алгоритм работы выглядит так:

![gan1](https://www.tensorflow.org/tutorials/generative/images/gan1.png)

1. Генератор создает несколько изображений на основе рандомных векторов
2. В дискриминатор передаются изображения генератора, а также несколько изображений из обучающей выборки. Дискриминатор должен понять, на какой картинке настоящая картинка из датасета, а на какой сгенерированная (это задача бинарной классификации)
3. Так как мы знаем какая картинка сгенерированная, а какая настоящая, то мы можем посчитать ошибку дискриминатора и методом обратного распространения ошибки обновить веса дискриминатора.
4. Чтобы посчитать ошибку генератора, нужно посмотреть какие картинки, сгенерированные генератором, были помечены как взятые из датасета, а какие сгенерированными. Если картинка помечена как взятая из датасета, то генератор получает "плюс", а если как сгенерированная, то "минус". На основе этого мы можем посчитать ошибку и обновить веса генератора.

Генератор и дискриминатор работают совместно. Задача генератора сгенерировать такое изображение, чтобы дискриминатор не догадался, что это изображения сгенерировано, в то время как его задача максимально точно отличать сгенерированные изображения от реальных.

![gan2](https://www.tensorflow.org/tutorials/generative/images/gan2.png)

Основная проблема, с которой сталкиваются такие нейросети - очень долгое обучение. Перед тем как нейросеть начнет выдавать стабильно хороший результат, может пройти не один десяток эпох, а само обучение может занимать часы и даже дни. Все дело в том, что ошибка, которую мы считаем для обеих нейросетей, не слишком-то информативна, но ничего лучше придумать нельзя. Представьте, что вам поручили научиться писать китайские иероглифы без словаря, а после каждой написанной страницы вам либо говорят, что где-то была ошибка, либо говорят, что все верно. Но есть еще одна деталь - человек, который вас проверяет, на самом деле тоже не знает китайский, а всего лишь подглядывает в словарь и сравнивает написанное вами с тем, что он видит там. Как думаете, сколько времени ушло бы на то, чтобы при такой схеме научиться писать иероглифы? Месяц? Год? А все из-за того, что ошибка, на которую указывают, не очень информативна.

Также важно, чтобы нейросети обучались с одинаковой скоростью. Если дискриминатор быстро поймет как отличать настоящую картинку от сгенерированной, то генератор не сможет ничему научиться (произойдет затухание градиента). Опять же, вспоминаем обучение китайской грамоте - если вам всегда будут говорить, что написанный вами текст не похож на настоящий, то вы ничему не научитесь ни за год, ни за 10 лет.

Для того, чтобы попробовать поработать на практике с генеративно-состязательными нейросетями, можно пройти [туториал от tensorflow](https://www.tensorflow.org/tutorials/generative/dcgan), где вы научитесь генерировать изображения рукописных цифр на основе датасета MNIST.
