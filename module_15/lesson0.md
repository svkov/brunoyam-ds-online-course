# Введение в нейронные сети

Анализ данных обрел свою популярность в последние года именно благодаря нейронным сетям. Чтобы анализировать большие данные и находить в них очень сложные закономерности, нужны сложные модели с миллионами параметров. Нейронные сети - лучший кандидат для такой задачи. Благодаря нейронным сетям машина на дороге может отличать пешеходный переход от дорожного знака, а голосовой ассистент может не только понять вопрос, но и дать ответ, синтезируя речь, похожую на человеческую. Используя простые модели, которые мы разбирали ранее, решать эти задачи очень сложно, а некоторые в принципе невозможно. Однако нейронные сети - не панацея. У них есть несколько очень серьезных проблем.

Во-первых, нейронные сети неинтерпретируемы. Для того, чтобы понять, что это значит, давайте вспомним линейную регрессию. Каждому признаку мы сопоставляем какой-то вес, складываем все и получаем ответ. Если модель ошибется на каких-то данных, мы сможем понять, почему так произошло и исправить конкретный случай. В случае с нейросетями так сделать не получится, так как в них миллионы параметров и пока что не существует инструментов, которые позволяли бы однозначно интерпретировать любую нейронную сеть. Ведется активная работа в этом направлении и сейчас появляются первые инструменты, которые позволяют заглянуть "под капот" нейронных сетей и как-то визуализировать как именно они принимают решения, но пока что это работает далеко не для всех видов нейросетей и это все еще нерешенная задача. Пошли бы вы на операцию, которую бы делал робот, если бы знали, что никто не знает, что на самом деле за программа в нем написана? Было бы вам комфортно ехать в машине с автопилотом, алгоритм работы которого до конца неизвестен? В то же время мы до конца не понимаем как работает человеческий мозг и мы доверяем другим людям оперировать нас и возить на машине. Интерпретируемость моделей машинного обучения - это важный этический вопрос, на который сейчас нет ответа.

Во-вторых, нейронные сети - это дорого. Недавно Яндекс выложил модель [YaLM 100B](https://habr.com/ru/company/yandex/blog/672396/), в которой 100 миллиардов параметров. Модель позволяет понимать текст, а также генерировать ответы. На этой языковой модели работает Поиск и Алиса. Когда такая модель попадает в Интернет, все сразу хотят потестировать ее на своем комьютере, но тут мы сталкиваемся с главной проблемой. Для того, чтобы запустить эту модель, необходимо 200 Гб видеопамяти, а это примерно 4 видеокарты A100 или 8 видеокарт V100. В домашних условиях запустить такую модель точно не получится, но предположим вы маленькая компания, которая смогла собрать деньги на аренду такого железа. Для того, чтобы эта нейросеть решала вашу задачу, вам будет необходимо дообучить ее на ваших данных для ваших целей. То есть, этих видеокарт будет недостаточно и потребуется больше ресурсов. Обучение исходной модели происходило на кластере из 800 видеокарт A100, а такие мощности могут себе позволить очень малое количество компаний. Мало того, что потребовалось 800 видеокарт, так еще и обучение проходило 65 дней. Также сейчас отдельное внимание уделяется вопросам экологичности машинного обучения. Обучение больших моделей нейронных сетей требует много электричества, которое в основном вырабатывается из газа и угля. По теме Green AI проводят конференции и пишут научные статьи, но в целом это довольно новая область.

В-третьих, нейросети сложно разрабатывать. Из-за огромного количества параметров они легко переобучаются, и нужно очень строго контроллировать обучение. Подбор гиперпараметров и правильной архитектуры тут намного важнее, чем в классических методах машинного обучения. Если для условного градиентного бустинга подобрав оптимальные гиперпараметры мы смогли улучшить качество на 10-15%, то для нейросети изменив оптимальные параметры на маленькое значение, модель может либо начать переобучаться, либо вообще оптимизационный метод начнет расходиться.

В-четвертых, для работы нейросетей нужно большое количество данных, которое есть далеко не у всех компаний. Можно обучить нейросеть на тысяче примеров, но ее точность будет сравнима с классическими моделями, но обучение будет идти дольше и сложнее настраиваться. Ощутимый прирост в точности можно получить на датасетах с миллионами примеров.

В последнее время нейросетями стали решать задачи, для которых они не предназначены и которые проще решить обычными классическими моделями. Если у вас мало данных, вы ограничены по ресурсам или модель должна работать в онлайне, то вы, скорее всего, не будете использовать нейросети. Однако если вы боретесь за каждую долю процента точности и у вас есть для этого данные и ресурсы, то нейросети - это ваш главный инструмент. В этом модуле мы обсудим какие задачи можно решать при помощи нейросетей, изучим их виды, а также узнаем какими библиотеками пользуются для их разработки.

## Библиотеки для разработки нейронных сетей

Сейчас существует два основных фреймворка - Tensorflow и Pytorch.

- [Tensorflow](https://github.com/tensorflow/tensorflow) - разработан Google. Имеет множество функций и проще для начинающих. В состав tensorflow входит библиотека Keras, которая позволяет обучать нейросети, используя минимальное количество кода. Имеет две версии - первую (1.x) и вторую (2.x), поэтому в туториалах нужно обращать внимание, чтобы они были для второй версии. Мы будем использовать эту библиотеку.
- [Pytorch](https://github.com/pytorch/pytorch) - разработан Facebook. Имеет еще больше функций, чем tensorflow, но немного сложнее для старта.

Обе библиотеки очень популярны и используются повсеместно. Tensorflow появился раньше и более популярен, но Pytorch более гибок.

Также есть библиотеки, которые были популярны ранее, но сейчас ими пользуются редко: Theano, Caffe, Lasange и другие.
