# Сверточные нейронные сети

Обучая многослойный перцептрон можно аппроксимировать любую функцию, но для некоторых задач проще использовать другую архитектуру сети. Например, для задач с аудио и картинками хорошо подходит сверточная нейронная сеть (CNN).

## Как это работает

Допустим у нас есть датасет изображений, в котором есть коты и собаки и мы хотим построить классификатор, который бы определял какое животное изображено на фотографии. Как бы мы решали такую задачу ничего не зная о машинном обучении? Можно было бы написать какой-то сложный алгоритм определения мордочки, лап и хвоста, а затем искать разницу между котами и собаками. Такой алгоритм был бы сложным с точки зрения написания кода и его нельзя было бы обобщить на другие похожие виды задач, но в целом этот подход верный. Мы попытались найти какие-то отличительные черты - паттерны - на изображении, которые отличают первый класс от вторго. Теперь главный вопрос - как автоматизировать процесс поиска этих паттернов? Было бы здорово сделать какое-то преобразование над изображением, пропустить его через какой-то фильтр, и получить одну-единственную деталь, которая нас интересует. Такими фильтрами являются свертки.

**Свертка** (convolution) - особая операция над изображением, когда мы скользящим окном идем по изображению и каждый кусочек изображения пропускаем через фильтр, который называется **ядром свертки**. Ядро свертки зафиксировано для всего изображения и является всего лишь матрицей. Пропустив изображение через правильно подобранную свертку можно получать нужные паттерны на изображении. Размер ядра обычно берут 3х3 и коэффициенты ядра будут теми параметрами, которые мы будем оптимизировать.

![conv](https://habrastorage.org/r/w1560/webt/v9/k2/kc/v9k2kc8ng4nrhryunr3wr6l5brg.png)

Как применить свертку? Для этого нужно умножить каждое значение ядра на соответствующее значение на изображении, а затем сложить все получившееся значения.

![conv_anim](https://habrastorage.org/webt/o0/zh/rz/o0zhrzr_ml2tgsfmvl-mcrxjmbq.gif)

После этой операции некоторые элементы матрицы могут оказаться отрицательными и нам это не подходит, поэтому обычно применяют функцию активации ReLU, которая просто заменяет отрицательные значения нулем.

У свертки есть несколько важных параметров. Во-первых, многое зависит от размера ядра, но чаще всего берут 3х3. Во-вторых, мы можем по-разному начинать обход изображения.

![conv_types](https://habrastorage.org/r/w1560/webt/rs/z8/ly/rsz8lyxtufyifb_jvfv82h7zq0e.png)

Если начать обход как показано на первой картинке и закончить также, то итоговое изображение будет меньшей размерности. Например, если изображение было 28х28 и мы прошли по нему сверткой 3х3, то изображение станет размера 26х26. Если применить свертку несколько раз, то изображений станет совсем маленьким. Чтобы так не случалось, обычно используют подход как на второй картинке. Изображение как бы помещают внутрь пустой рамки (рамка заполнена нулями), и начинают проходить окном вместе с такой рамкой. Ширина и высота такой рамки задается параметром **отступа** (padding). На картинке ниже показан пример для ядра 2х2, картинки 3х3 и отступа 1.

![conv_padding](https://miro.medium.com/max/512/1*5rLRx19ot0QggMn9teY14Q.png)

Еще один важный параметр - **шаг перемещения окна** (stride). Если изображение очень большое и не сильно меняется, то мы можем ускорить вычисления, сдвигая скользящее окно, например, на два пикселя вместо одного.

![conv_stride](https://habrastorage.org/r/w1560/getpro/habr/upload_files/d1f/c56/234/d1fc56234e99e9c7b880133a8ad29d4a.png)

На этом со сверткой все. Что дальше? Чаще всего на всем изображении интересующая нас деталь будет занимать очень малое пространство, а все остальное изображение будет зашумлено после операции свертки. Надо как-то сжать изображение. Чтобы сделать это по-умному, давайте воспользуемся операцией пулинга.

**Пулинг** (pooling) - операция над изображением, когда мы уменьшаем его размер за счет объединения соседних пикселей в один. Выделяет несколько видов пулинга - MinPooling, MaxPooling, AveragePooling. Для начала разберемся с последним. Будем идти по изображению скользящим окном размера 2х2 и на каждом шаге будем искать максимальное значение в матрице и записывать его в новое изображение. Так исходное изображение размера 28х28 превратится в изображение размера 14х14.

![pooling](https://habrastorage.org/r/w1560/webt/0u/ji/tm/0ujitma2xn_ndxqswj5s31je2am.png)

В случае MinPooling-а считается минимум на каждом шаге, а в случае AveragePooling-а считается среднее. Чаще всего используется MaxPooling, так как нам нужно искать самые контрастные пиксели на изображении.

Применив свертку и пулинг, мы получим уменьшенное изображение с выделенным паттерном. Вспомним теперь про фотографии животных. Чтобы отличить кота от собаки нам нужно знать множество деталей, а не какую-то одну, поэтому было бы логично использовать не одну свертку, а несколько. Тогда бы одна свертка находила хвост, другая мордочку, а третья лапы. На практике обычно используют большое количество сверток - 32/64/128. Этого достаточно для большинства изображений. Также операцию свертка + пулинг обычно повторяют несколько раз, потому что после первой фильтрации не всегда получается то, что хотелось бы.

Давайте подытожим как работает сверточная нейронная сеть. Сначала нам нужно применить операцию свертки. Зная ядро свертки, мы можем пройти скользящим окном по изображению и сформировать новое, каждый пиксель которого был получен в результате пропускания кусочка исходного изображения через фильтр. Далее нам нужно применить операцию пулинга, чтобы сделать изображение меньше и нужная деталь была более ярко выражена. Такую операцию мы можем повторять несколько раз

![pic1](https://drive.google.com/uc?export=view&id=1CgvgrVTOdvsT4LzltnayZ0SxPuUyMEpJ)

![pic2](https://drive.google.com/uc?export=view&id=1esI1fEimHm7EPe-c7c82knkofwyDxflY)

После того как мы посчитали нужное количество раз свертки и пулинги, можно применить обычный перцептрон, чтобы решить задачу. Вся информация об изображении, нужная нейросети, будет находиться в свертках, а перцептрон в последнем слое просто определит какая из сверток лучше подходит для решения конкретной задачи. В больших нейросетях, которые обучаются на терабайтах данных, обычно в последних слоях хранится вся информация о картинке и нейросеть уже знает, что на ней изображено. Такую нейросеть можно использовать для решения большого класса задач. Нужно только правильно вытащить информацию из последних слоев и для этого обычно "замораживают" первые слои и дообучают последние слои нейросети на небольшом количестве данных, чтобы модель поняла, какой ответ от нее ожидается. Такой подход называется **fine-tuning** или **дообучение модели**, мы посмотрим как это работает на практике немного позже.

## Обучение сверточных нейросетей

Как мы уже знаем, обычно сразу используют несколько сверток параллельно, поэтому говорят о том, что используется сверточный слой. Во время обучения подбираются коэфициенты ядер. Обучение происходит точно также при помощи метода обратного распространения ошибки и формулы не будут кардинально отличаться от тех, что мы разбирали в прошлом уроке, но вообще [вот тут](https://habr.com/ru/post/348028/) можно прочитать об этом. Можно подумать, что обучение сверточных нейросетей будет идти дольше, так как здесь у нас намного больше параметров. На самом деле, за счет того, что мы можем параллельно считать все свертки на разных процессорах, обучение таких сетей проходит довольно быстро на видеокарте (GPU). Если же использовать CPU, то, конечно, обучение займет больше времени.
