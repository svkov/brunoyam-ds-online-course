# Основные теоремы статистики

Почему то, что мы придумали в теории вероятности, вообще работает и для статистики с ее выборками и неполной информацией о генеральной совокупности?

Есть две теоремы, которые связывают статистику и теорию вероятности, позволяя делать выводы по данным.

## Закон больших чисел

Говоря простым языком эта теорема о том, что чем больше у нас выборка, тем более точно мы можем сделать по ней выводы.

Переходя к частности, возьмем нормальное распределение с матожиданием 10 и дисперсией 1. Сгенериуем сначала выборку размера 10, затем размера 100 и размера 1000. На каждом шаге посчитаем среднее значение выборки и сравним с матожиданием. С увеличением количества наблюдений ошибка будет уменьшаться.

Итак, можно сформулировать закон больших чисел:

$$\bar{X_n} \rightarrow \mu, n \rightarrow \infty$$

## Центральная предельная теорема

В статистике мы активно пользуемся нормальным распределением. Откуда вообще оно берется и почему оно так важно?

Давайте возьмем какое-то распределение максимально не похожее на нормальное. Например, можно взять равномерное распределение на отрезке $[0, 1]$.

Сгенерируем много выборок длины $n=2$, для каждой выборки посчитаем среднее значение и отобразим это на графике

![uniform_2](../images/uniform_clt_n_2.png)

Увеличим $n$ до 30 и проделаем эксперимент заново.

![uniform_30](../images/uniform_clt_n_30.png)

На что же это похоже...? На нормальное распределение! Чем больше будет $n$, тем более будет похоже на нормальное распределение.

Если мы возьмем несколько выборок из любого другого распределения и рассмотрим распределение средних значений, то всегда будем получать распределение, стремящееся к нормальному, при увеличении длин выборок.

Важным требованием этой теоремы является независимость случайных величин (выборок длины $n$). Если они будут зависимы, то нормальное распределение может и не получиться.
