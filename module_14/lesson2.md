# Способы векторизации текстов

## Bag of word

В прошлом уроке мы познакомились со способом векторизации BOW. Давайте посмотрим как им пользоваться при помощи Python. В библиотеке scikit-learn он называется `CountVectorizer`.

**добавить пример**

```python

```

## TF-IDF

Проблема BOW заключается в том, что некоторые общеупотребительные слова могут встречаться во всех документах и модель будет переобучаться на них. Частично эта проблема решается на уровне фильтрации стоп-слов, но некоторые слова могут не быть стоп-словами, но встречаться почти в каждом документе и не нести никакой смысловой нагрузки.

Давайте будем штрафовать слово, если оно находится сразу во многих документах. Так, слово, которое встретилось всего в 5 документах будет нести больше смысла, чем слово, которое встретилось в 100 документах. Такой подход назвали **TF-IDF (term frequency - inverse document frequency)**.

TF-IDF считается так:

$$TFIDF = TF*IDF$$

- TF - term frequency - сколько раз слово встретилось в документе, как мы и считали в BOW
- IDF - inverse document frequency - $\frac{1}{d}$, где $d$ - количество документов, в которых встретилось это слово

TF-IDF позволяет сформировать такой вектор документа, в котором у каждого слова будет свой вес. Далее, например, модель логистической регрессии может каждому слову проставить свой вес и принять к какому классу будет принадлежать весь документ.

**добавить пример

```python

```

## Word2Vec

Оба метода, упомянутых выше, позволяли сделать вектор, который бы отражал суть какого-то документа. Однако мы не можем с их помощью найти похожие отдельные слова или сравнить "непохожесть" слов. Для того, чтобы решать такую задачу, существует метод Word2Vec.

Суть метода состоит в том, чтобы отобразить каждый токен в пространстве фиксированной размерности. Например, давайте представим, что у нас есть коллекция из миллиона документов, и мы хотим каждое слово отразить в виде вектора в 50-мерном пространстве. Для поиска близких по смыслу слов можно использовать косинусное расстояние.

Косинусное расстояние - это косинус угла между векторами:

$$cos(A, B) = \frac{(A, B)}{|A||B|}$$

Что для нас это означает? Мы можем находить как слова соотносятся между собой.

Например, если мы возьмем обученный Word2Vec и найдем векторы слов "король", "мужчина" и "женщина", то посчитав "женщина" + "мужчина - король" и найдя ближайшую точку, мы найдем "королева".

**вставить картинку**

Звучит как магия! Но именно так и работает Word2Vec. Алгоритм определяет значения слов на основе контекста. Контекст - это слова, которые окружают нужно нам слово. Обычно для того, чтобы как-то повлиять на обучение Word2Vec используют ширину окна, то есть на сколько слов слева и справа от текущего нужно обращать внимание алгоритму. Чем шире окно, тем больше контекста сможет запомнить модель, но в тоже время при широком окне будет попадать много шума.

[Подробнее про Word2Vec можно почитать здесь](https://habr.com/ru/post/446530/)

Давайте посмотрим как это работает на практике.

**вставить пример**

```python

```
