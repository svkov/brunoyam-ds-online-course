# Общий принцип обработки естественного языка

Задачи, связанные с анализом текстов, встречаются повсюду - машинный перевод, умные чат-боты, классификация текстов, веб-поиск. Весь класс задач называют **обработкой естественного языка** или **natural language processing (NLP)**.

Обработка естественного языка отличается от обработки обычных табличных данных. Когда мы разбирали типы данных, то говорили, что данные разделяют на числовые и категориальные. Текстовая информация принципиально отличается от других типов данных.

Что такое текст? Это набор слов, знаки препинания и спецсимволы типа `\n` и `\t`. Самое полезное на первый взгляд - это слова, поэтому первое, что мы сделаем - это уберем пунктуацию, и приведем все слова к нижнему регистру.

Как убрать знаки препинания? Если мы знаем какие именно знаки препинания могут встретиться в тексте, то мы можем использовать функцию `replace()`:

```python
text = "текст, текст."
text = text.replace(",", "").replace(".", "")
```

Однако если знаков препинания станет больше, то уже будет неудобно писать код в таком стиле. Поэтому обычно знаки препинания убирают при помощи регулярных выражений. Давайте сначала разберемся с тем, что это такое, а потом вернемся к обработке текстов.

## Регулярные выражения

![regex](../images/regex.jpeg)

Иногда нам нужно искать в тексте какие-то повторяющиеся кусочки. Например, как на картинке, нужно найти адрес в тексте, либо может быть нужно найти все телефонные номера, е-мейлы и т.д. Можно написать простой код из нескольких циклов, который будет искать нужную информацию, но иногда написать регулярное выражение быстрее, проще и эффективнее.

Итак, регулярное выражение - это строка, в которой задается шаблон поиска каких-то подстрок. Например, если мы хотим найти только английские буквы в тексте, то мы напишем такое регулярное выражение: `[a-z]+`. В квадратных скобках мы можем указать те значения, которые мы ищем, но если бы мы указали `[a-z]`, то каждая буква нашлась бы по отдельности. Если мы хотим искать непрерывную последовательность букв (например, слова), то нам нужно зациклить этот процесс. Для этого можно добавить в конце `+`.

[Вот здесь можно попрактиковаться с регулярными выражениями](https://regex101.com/)

Регулярные выражения можно использовать в Python при помощи модуля `re`. Две самые полезные функции - `search` и `sub`:

```python
import re

text = "some, text! ()"

is_found_text = re.search(r"[a-z]", text) # ищем латинскую букву в строке
if is_found_text:
    ...

clear_text = re.sub("[,!()]", "", text) # убираем ненужные символы
```

В нашем случае нужно убрать все символы, которые не относятся к буквам и пробелам. Давайте для начала научимся выбирать буквы и пробелы. Буквы можно распознать при помощи `\w` (тоже самое, что `[a-zA-Z0-9_]`), а пробелы при помощи `\s` (тоже самое, что `[\r\n\t\f\v  ]`). Если нам нужны и буквы и пробелы, то можно написать выражение `[\w\s]`, которое вернет весь текст, кроме пунктуации. Значит, нам нужно взять все символы, которые не входят в это выражение и убрать их. Чтобы сделать логическое отрицание, можно использовать `^`, то есть получим выражение `[^\w\s]`. А затем нужно найти все такие вхождения в строке и заменить их на пустую строку. Сделаем это при помощи Python:

```python
import re

text = "Знаки препинания важны для русского языка, но мешают исследователям данных."
text = text.lower() # приведем к нижнему регистру
text = re.sub(r'[^\w\s]', '', text) # уберем пунктуацию

print(text) # знаки препинания важны для русского языка но мешают исследователям данных
```

Задачи, которые можно решать при помощи регулярных выражений, бесчисленны, но их рассмотрение выходит за пределы нашего курса. Почитать больше про регулярные выражения можно [здесь](https://habr.com/ru/post/349860/).

## Векторизация текстов

Итак, мы научились приводить тексты к одному виду. Компьютер умеет хорошо работать с числами, поэтому давайте превратим текст в вектор.

Давайте рассмотрим задачу классификации документов. Пусть у нас есть датасет, в котором $n$ текстов (документов) и для них известны категории. Например, есть новостные статьи и их темы. Нужно построить модель, которая по заданному тексту определит его тему.

Давайте будем решать задачу так. Для каждого документа посчитаем сколько раз в него входит каждое из слов в датасете и будем использовать такие векторы как вектор $X$ в задаче классификации. Например, далее мы можем использовать логистическую регрессию или наивный байесовский классификатор.

Например, пусть у нас есть датасет из 3 текстов:

- 'Это первый документ.'
- 'Это второй документ среди всех документов.'
- 'А вот и третий.'
- 'Вот и кончились документы.'

Построим вот такую матрицу:

-/- | вот | всех | второй | документ | документов | документы | кончились | первый | среди | третий | это
|-|-|-|-|-|-|-|-|-|-|-|-|
Это первый документ. | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1
Это второй документ среди всех документов. | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 1
А вот и третий. | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0
Вот и кончились документы. | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0

Каждой строке соответствует один документ, а каждому столбцу одно слово (токен). Такой подход к векторизации текстов называется **Bag of Word (BOW)**.

В чем минус такого подхода? Если внимательно посмотреть на таблицу, то можно заметить, что есть много слов, которые повторяются из-за разных окончаний - "документ", "документов", "документы". Чтобы избежать такого поведения, нужно как-то предобработать текст до того как запускать BOW. Для этого можно либо попробовать привести слово к начальной форме (этот подход называется **лемматизация**), либо можно обрезать окончание (этот подход называется **стемминг**).

## Стемминг

Стемминг позволяет избавляться от слов-дубликатов за счет обрезания окончаний. Такие методы работают не очень точно, но зато быстро. Если данных очень много, то стоит использовать этот метод.

Самый известный стеммер для русского языка - [стеммер Портера](https://gist.github.com/Kein1945/9111512). Он работает на регулярных выражениях, определяя часть речи и откидывая окончание.

Обратите внимание, что стеммер применяется к каждому отдельному слову. Перед применением нужно документ разбить на слова, затем к каждому слову применить стеммер и склеить все обратно.

Вот так будет выглядеть пример после применения стемминга:

-/- |вот | всех | втор | документ | конч | перв | сред | трет | эт
|-|-|-|-|-|-|-|-|-|-|
эт перв документ | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1
эт втор документ сред всех документ | 0 | 1 | 1 | 2 | 0 | 0 | 1 | 0 | 1
а вот и трет | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0
вот и конч документ | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0

**добавить пример кода**

```python

```

## Лемматизация

Лемматизация позволяет приводить слово к начальной форме. Существует множество различных лемматизаторов для разных языков, их разрабатывают специалисты по математической лингвистике и детально разбираться с тем как это работает обычно не нужно. Главное - знать какие есть лемматизаторы для английского и русского языка и как их использовать

**добавить пример**

```python

```

## Стоп-слова

Если вы были внимательны, то могли заметить, что при векторизации пропадали все предлоги. Почему так произошло? Дело в том, что предлоги, артикли и союзы не несут полезной информации о документе и при этом встречаются почти в каждому документе. Модель может переобучиться на стоп-словах и работать неправильно. Для большинства языков списки таких стоп-слов давно известны и нам нужно лишь убирать их из наших документов.

**добавить пример**

```python

```
